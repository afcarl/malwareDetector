#!/usr/bin/env python3
import sys
import os
import subprocess
import json
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import QuantileTransformer
from utils.BinaryTools.bin2bytes import bin2bytes
import tensorflow as tf
import pandas as pd
import numpy as np
import tensorflow.contrib.layers as layers
from analyzers.filetype.filetype import *

normalizers = [lambda x : StandardScaler().fit_transform(x.astype(np.float64)),
	    lambda x : MinMaxScaler().fit_transform(x.astype(np.float64)),
	    lambda x : MaxAbsScaler().fit_transform(x.astype(np.float64)),
	    lambda x : RobustScaler(quantile_range=(25, 75)).fit_transform(x.astype(np.float64)),
	    lambda x : QuantileTransformer(output_distribution='uniform').fit_transform(x.astype(np.float64)),
	    lambda x : QuantileTransformer(output_distribution='normal').fit_transform(x.astype(np.float64)),
	    lambda x : Normalizer().fit_transform(x.astype(np.float64))]

extractors = {
'bin':[
    './analyzers/api/api_binary.py',
    './analyzers/entropy/entropy_binary.py',
    './analyzers/peheader/header.py',
    './analyzers/importTable/iat.py',
    './analyzers/opcode/opcode.py',
    './analyzers/register/register.py',
    './analyzers/section/section.py',
    './analyzers/string/str.py'
],
'bytes':[
    './analyzers/bytesngram/extract_1_gram.py',
    './analyzers/bytesngram/extract_2_gram.py',
    './analyzers/img/img1.py',
    './analyzers/img/img2.py'
]
}

# sys.path.append('./analyzers/filetype')
os.environ['PYTHONPATH'] = "./analyzers/filetype"

# In[4]:
# traindata , trainlabel, testdata, testlabel = fr(feature_choice=range(12), normalize_choice=[6, 6, 6, 6, 6, 6, 0, 1, 6, 6, 0, 5], dir_name='features').read()

# testlabel = testlabel.reshape((-1,1))
# testlabel = np.hstack((testlabel,testlabel))

# for tt in testlabel:
#     if tt[0] == 1:
#         tt[1] = 0
#     else:
#         tt[1] = 1

def usage():
    print('Usage: {} filename/dirname'.format(sys.argv[0]))

def read_samples(path):
# read_samples:
# path: a path to a executable file or to a directory containing executables
# return a list of samples' filenames
    if os.path.isfile(path):
        return [path]
    else:
        return [os.path.join(path, x) for x in os.listdir(path)]


def extracFeature(filepaths):
    retval = []
    filelist = []
    for path in filepaths:
        feature = []
        filename = os.path.basename(path)
        bytesfile = "/tmp/{}.bytes".format(filename)
        if os.path.isfile(path):
            try:
                f = File(path)
                if not f.is_pe():
                    continue
                bin2bytes(path, bytesfile)
                for filetype in ['bin', 'bytes']:
                    for e in extractors[filetype]:
                        if filetype == 'bin':
                            command = (e, path)
                        else:
                            command = (e, bytesfile)
                        # print(command)
                        output = subprocess.check_output(command, env = os.environ).decode('utf8')
                        output = json.loads(output)
                        assert output['stat'] == "success"
                        assert output['messagetype'] == 'list'
                        feature += output['message']
                assert len(feature) == 67694
                retval.append(feature)
                filelist.append(path)
            except Exception as e:
                print(path, e, file=sys.stderr)
            finally:
                os.remove(bytesfile)
            
    return np.array(retval, dtype='float32'), filelist

def normalize(features):
    normalize_choice = [6, 6, 6, 6, 6, 6, 0, 5, 6, 6, 0, 1]
    len_each_feature = [796, 256, 256, 256, 93, 29, 24, 32, 256, 65536, 52, 108]
    end = 0
    for i in range(len(normalize_choice)):
        start = end
        end = start + len_each_feature[i]
        features[:,start:end] = normalizers[normalize_choice[i]](features[:,start:end])
    return features

def predict(input_sample):
    feature_size = np.shape(input_sample)[1]
    sample_size = np.shape(input_sample)[0]
    class_size = 2

    print(sample_size,feature_size)
    layer_size = 1024
    reg = 0.1

## build graph

    X = tf.placeholder(tf.float32,[None, feature_size], name='X')
# Y = tf.placeholder(tf.float32,[None, class_size], name='Y')

    layer1 = layers.fully_connected(inputs=X, num_outputs=layer_size, activation_fn=tf.nn.relu, weights_regularizer = layers.l2_regularizer(scale=reg))

    layer2 = layers.fully_connected(inputs=layer1, num_outputs=layer_size, activation_fn=tf.nn.relu, weights_regularizer = layers.l2_regularizer(scale=reg))
    fc = layers.fully_connected(layer2, feature_size, tf.nn.relu)

    w = tf.Variable(tf.random_normal(shape=[feature_size, class_size], stddev=0.01), name='weights')
    b = tf.Variable(tf.zeros([1, class_size]), name="bias")
    logits = tf.matmul(fc, w) + b

    saver = tf.train.Saver()
    with tf.Session() as sess:
        saver.restore(sess, "./model/model.ckpt")
        predict_prob = logits.eval({X: input_sample})
        predictions = tf.argmax(predict_prob, 1 ).eval()
        # result = np.equal(np.argmax(predict_prob ,1), np.argmax(testlabel, 1)).astype('float32')

    return predictions

def main():
    if len(sys.argv) != 2:
        usage()
        sys.exit(-1)

    filenames = read_samples(os.path.abspath(sys.argv[1]))
    features, filelist = extracFeature(filenames)
    assert len(features) == len(filelist)
    features = normalize(features)
    result = predict(features)

    for i in range(len(filelist)):
        print(filelist[i], result[i])


if __name__ == '__main__':
    main()
